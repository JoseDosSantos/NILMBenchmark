{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T15:17:23.548091Z",
     "start_time": "2021-02-08T15:17:23.536088Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 12]\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:32:35.289676Z",
     "start_time": "2021-02-08T14:32:35.252645Z"
    }
   },
   "outputs": [],
   "source": [
    "!cd C:\\Users\\Josef\\Google Drive\\Uni\\Master\\3 Wintersemester 20-21\\Seminar Information Systems\\Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:32:35.802068Z",
     "start_time": "2021-02-08T14:32:35.785065Z"
    }
   },
   "outputs": [],
   "source": [
    "AMPds_PATH = \"C:/Users/Josef/Google Drive/Uni/Master/3 Wintersemester 20-21/Seminar Information Systems/Contribution/data/AMPds/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T00:06:42.251000Z",
     "start_time": "2021-02-10T00:06:41.081737Z"
    }
   },
   "outputs": [],
   "source": [
    "amp = pd.read_csv(AMPds_PATH + \"Electricity_P.csv\")\n",
    "weather = pd.read_csv(AMPds_PATH + \"Climate_HourlyWeather.csv\")\n",
    "\n",
    "amp.set_index(pd.to_datetime(amp.UNIX_TS, unit=\"s\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T00:18:06.506730Z",
     "start_time": "2021-02-10T00:18:06.493727Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataStreamerNilm:\n",
    "    \"\"\"Returns batches of a given dataset.\n",
    "\n",
    "    Takes a given dataset, optionally enriches it with additional data and \n",
    "    returns an iterator over that dataset with the given batch size. Note that\n",
    "    this function applies no preprocessing, so the input data needs to be \n",
    "    processed beforehand.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        mains_col: str,\n",
    "        appliance_cols: Union[str, list],\n",
    "        batch_size: int = 8192,\n",
    "        window_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        chunksize: int = -1,\n",
    "        random_state: int = None,\n",
    "        validation_size: float = 0.2\n",
    "    ):\n",
    "        \"\"\"Initialize NILM data streamer.\n",
    "\n",
    "            Args:\n",
    "            dataset: pd.DataFrame of mains and appliance data.\n",
    "              TODO: Load file from disk.\n",
    "            mains_col: Name of the columns containing the mains readings.\n",
    "            appliance_col: Either single name or list of appliance names to \n",
    "              return.\n",
    "            batch_size: Number of datapoints returned.\n",
    "            window_size: In case sequential training data is needed, each \n",
    "              batch item consists of a time window with given length. Leave at \n",
    "              1 to return independent singular observations.\n",
    "            shuffle: Shuffle data before yielding. If window length is given,\n",
    "              the data is first split into window-sized continuous chunks and\n",
    "              then shuffled to preserve order.\n",
    "              TODO: How to handle this with window batches.\n",
    "            chunksize: Currently not implemented. Number of observations to \n",
    "              load from disk.\n",
    "              TODO: If file is loaded from memory, enable chunkwise loading.\n",
    "            random_state: Use to get reproducable shuffling results.\n",
    "            validation_size: percentage of the data to be used for validation\n",
    "\n",
    "        Yields:\n",
    "            An iterable over the input dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        self.mains_col = mains_col\n",
    "        self.appliance_cols = appliance_cols\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.shuffle = shuffle\n",
    "        self.chunksize = chunksize\n",
    "        self.random_state = random_state\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        \n",
    "        # We only need to keep mains + selected appliances in memory\n",
    "        if type(appliance_cols) is str:\n",
    "            self.dataset = dataset.filter([mains_col, appliance_cols])\n",
    "        else:\n",
    "            self.dataset = dataset.filter([mains_col] + appliance_cols)\n",
    "        \n",
    "        self.reset_iterator(self.dataset)\n",
    "        \n",
    "    def generate_batch(self):\n",
    "        target, features = next(self.dataset_iterator)\n",
    "        return target, features\n",
    "\n",
    "    def _dataset_to_list(self, data: pd.DataFrame, mains_col: str):\n",
    "        # Steps:\n",
    "        # 1 Split into rows or chunks\n",
    "        # 2 Create numpy arrays with format\n",
    "        # batch_size x window_length x appliances/data\n",
    "\n",
    "        mains = data[mains_col].values\n",
    "        appliances = data.drop(columns=[mains_col])\n",
    "\n",
    "    def _dataset_iterator(self, data:list):\n",
    "        \"\"\"\n",
    "        Yields batches of data. Expects list of batches, each containing two\n",
    "        arrays, one with mains data and one with corresponding features.\n",
    "        \"\"\"\n",
    "        for batch in data:\n",
    "            yield batch\n",
    "\n",
    "\n",
    "    def reset_iterator(self, data: pd.DataFrame) -> None:\n",
    "        \"\"\"Reset data streamer and empty sample cache\"\"\"\n",
    "        df_length_original, n_cols = data.shape\n",
    "\n",
    "        if self.window_size > 1:\n",
    "            # A bit hacky, but to make the reshape work we cut off a small part\n",
    "            # at the end so the dataset nicely divides into window_sized parts\n",
    "            cutoff = df_length_original % self.window_size\n",
    "            if cutoff > 0:\n",
    "                data = data[:-cutoff]\n",
    "        df_length = data.shape[0]\n",
    "        n_splits = df_length // self.window_size\n",
    "\n",
    "        # Reshape the data into window_sized parts\n",
    "        data = data.to_numpy().reshape((n_splits, self.window_size, n_cols))\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(data)\n",
    "        \n",
    "        # There might be a better way to make sure the data exactly divides into\n",
    "        # the given amount of batches, but probably not an issue with sufficient\n",
    "        # training samples.\n",
    "        batch_cutoff = n_splits % self.batch_size\n",
    "        if batch_cutoff > 0:\n",
    "            data = data[:-batch_cutoff]\n",
    "        \n",
    "        # Now separate the shuffled and windowed observations into target and\n",
    "        # feature lists.\n",
    "        # TODO: Maybe this step can be done before and both lists can instead\n",
    "        # be shuffled separately with same seeds.\n",
    "        target_list = []\n",
    "        feature_list = []\n",
    "        for window in data:\n",
    "            target, features = np.hsplit(window,[1])\n",
    "            target_list.append(target)\n",
    "            feature_list.append(features)\n",
    "        \n",
    "        # Finally split the data into batches, consisting of a list of target\n",
    "        # windows and a list of corresponding feature windows.\n",
    "        n_batches = len(target_list) // self.batch_size\n",
    "        batches = []\n",
    "        \n",
    "        # TODO: Create batch-indexes in a nicer way\n",
    "        for i in range(n_batches):\n",
    "            batches.append([target_list[i*self.batch_size:i*self.batch_size+self.batch_size],\n",
    "                            feature_list[i*self.batch_size:i*self.batch_size+self.batch_size]])\n",
    "        \n",
    "        self.dataset_iterator = self._dataset_iterator(batches)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T00:18:15.002427Z",
     "start_time": "2021-02-10T00:18:13.735430Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = DataStreamerNilm(\n",
    "    dataset = amp,\n",
    "    mains_col = \"MHE\",\n",
    "    appliance_cols = [\"FGE\", \"UNE\"],\n",
    "    shuffle=False,\n",
    "    window_size=8,\n",
    "    batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Point(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Point, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Conv2d(in_channels=1, \n",
    "                                     out_channels=30, \n",
    "                                     stride=(1, 1), \n",
    "                                     kernel_size=(10, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T11:39:16.997692Z",
     "start_time": "2021-02-10T11:39:16.479494Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dataloader, \n",
    "                 device=\"cpu\",\n",
    "                 validation_split=0.2):\n",
    "        \n",
    "        self.device = device\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "        # Initialize input and output embeddings (w_in and w_out)\n",
    "        self.center_model = nn.Embedding(vocab_size, embedding_dim).to(self.device)\n",
    "        self.context_model = nn.Embedding(vocab_size, embedding_dim).to(self.device)\n",
    "        \n",
    "        # Pass iterable of both center and context embeddings to optimizer\n",
    "        self.optim = torch.optim.Adam(params=chain(self.center_model.parameters(), self.context_model.parameters()),\n",
    "                                      lr=0.001,\n",
    "                                      betas=(0.9, 0.999),\n",
    "                                      eps=1e-08) \n",
    "        \n",
    "        self.loss = SkipGramLoss()\n",
    "\n",
    "    \n",
    "    def train(self, epochs=5):\n",
    "        \n",
    "        self.epochs=epochs\n",
    "        \n",
    "        print(f\"Training for {self.epochs} epochs started at {strftime('%H:%M:%S', localtime())}\")\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # Reset iterator (as the loader runs through its basket iterator completely during each epoch)\n",
    "            self.dataloader.reset_iterator()\n",
    "            \n",
    "            # Used for outputting running statistics\n",
    "            running_loss = 0.0\n",
    "            batch = 0\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    # Reset gradient\n",
    "                    self.optim.zero_grad()\n",
    "\n",
    "                    # Generate a batch and convert indices to tensor (needed for lookup in embedding layer)\n",
    "                    center, context, negative_samples = self.dataloader.generate_batch()\n",
    "                    center, context, negative_samples = torch.tensor(center), torch.tensor(context), torch.tensor(negative_samples)\n",
    "                    center, context, negative_samples = center.to(self.device), context.to(self.device), negative_samples.to(self.device)\n",
    "                    \n",
    "                    # Get embeddings from embedding_layers\n",
    "                    center_embed = self.center_model(center).to(self.device)\n",
    "                    context_embed = self.context_model(context).to(self.device)\n",
    "                    negative_samples_embed = self.context_model(negative_samples.long()).to(self.device)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = self.loss(center_embed, context_embed, negative_samples_embed)\n",
    "                    \n",
    "                    # Backprop\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Make a step\n",
    "                    self.optim.step()\n",
    "                    \n",
    "                    #Save running loss and batch count for output\n",
    "                    running_loss += loss.item()\n",
    "                    batch += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Perhaps not the cleanest way to do it, but once the dataloader iterated through all baskets an\n",
    "                    # error will be thrown which ends the epoch training loop\n",
    "                    break\n",
    "                    \n",
    "            print(f\"Finished epoch {epoch+1} at {strftime('%H:%M:%S', localtime())}\\t Loss: {running_loss/batch:9.6f}\")\n",
    "\n",
    "trainer = Trainer(dataloader=ts, \n",
    "                  device=\"cuda\",\n",
    "                  validation_split=0.2)\n",
    "\n",
    "trainer.train(epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
